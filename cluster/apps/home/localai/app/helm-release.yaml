apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: &appname localai
  namespace: home
spec:
  interval: 20m
  chart:
    spec:
      chart: app-template
      version: 1.5.0
      interval: 5m
      sourceRef:
        kind: HelmRepository
        name: bjw-s-charts
        namespace: flux-system
  # See https://github.com/bjw-s/helm-charts/blob/main/charts/library/common/values.yaml
  values:

    image: 
      repository: quay.io/go-skynet/local-ai
      tag: v1.14.2

    env:
    - name: THREADS
      value: 6
    - name: CONTEXT_SIZE
      value: 512
    - name: MODELS_PATH
      value: "/models"
    - name: IMAGE_PATH
      value: /tmp
    - name: BUILD_TYPE
      value: openblas
    - name: GO_TAGS
      value: stablediffusion
    - name: DEBUG
      value: "true"

    initContainers:
      download-model:
        image: busybox@sha256:560af6915bfc8d7630e50e212e08242d37b63bd5c1ccf9bd4acccf116e262d5b
        command: ["/bin/sh", "-c"]
        args:
          - |
            ## A simpler and more secure way if you have a way of staging an archive with the files you need
            #wget "https://s3.${SECRET_DEV_DOMAIN}/public/stablediffusion.tar" -P /tmp
            #tar -xzvf /tmp/stablediffusion.tar -C $MODELS_PATH
            #rm -rf /tmp/stablediffusion.tar

            ## A more general and less secure way that grab all the files from github
            ## Details here: https://github.com/go-skynet/LocalAI
            ## And here: https://github.com/lenaxia/stablediffusion-bins/releases/tag/2023.05.24
            mkdir $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/AutoencoderKL-256-256-fp16-opt.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/AutoencoderKL-512-512-fp16-opt.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/AutoencoderKL-base-fp16.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/FrozenCLIPEmbedder-fp16.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/UNetModel-256-256-MHA-fp16-opt.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/UNetModel-512-512-MHA-fp16-opt.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/UNetModel-base-MHA-fp16.param" -P $MODELS_PATH/stablediffusion_assets
            wget "https://github.com/EdVince/Stable-Diffusion-NCNN/raw/main/x86/linux/assets/log_sigmas.bin" -P $MODELS_PATH/stablediffusion_assets
            wget "https://raw.githubusercontent.com/EdVince/Stable-Diffusion-NCNN/main/x86/linux/assets/vocab.txt" -P $MODELS_PATH/stablediffusion_assets
            wget "https://github.com/lenaxia/stablediffusion-bins/releases/download/2023.05.24/UNetModel-MHA-fp16.bin" -P $MODELS_PATH/stablediffusion_assets
            wget "https://github.com/lenaxia/stablediffusion-bins/releases/download/2023.05.24/FrozenCLIPEmbedder-fp16.bin" -P $MODELS_PATH/stablediffusion_assets
            wget "https://github.com/lenaxia/stablediffusion-bins/releases/download/2023.05.24/AutoencoderKL-fp16.bin" -P $MODELS_PATH/stablediffusion_assets
            wget "https://github.com/lenaxia/stablediffusion-bins/releases/download/2023.05.24/AutoencoderKL-encoder-512-512-fp16.bin" -P $MODELS_PATH/stablediffusion_assets
        
            cat << "EOF" >> $MODELS_PATH/stablediffusion.yaml
            name: stablediffusion
            backend: stablediffusion
            asset_dir: stablediffusion_assets
            EOF

        env:
          - name: URL
            value: "https://gpt4all.io/models/ggml-gpt4all-j.bin"
          - name: MODELS_PATH
            value: "/models"
        volumeMounts:
          - name: models 
            mountPath: /models
        securityContext:
          runAsUser: 0

    persistence:
      models:
        enabled: true
        storageClass: local-path
        size: 30Gi
        type: pvc
        accessMode: ReadWriteOnce

    service:
      main:
        type: LoadBalancer
        ports:
          http:
            port: &port 8080

    ingress:
      main:
        enabled: true
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/icon: eos-icons:ai
          hajimari.io/info: Local AI
          hajimari.io/group: home
          cert-manager.io/cluster-issuer: "letsencrypt-production"
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: networking-chain-authelia@kubernetescrd
        hosts:
        - host: &uri ai.${SECRET_DEV_DOMAIN}
          paths:
          - path: /
            pathType: Prefix
        tls:
        - hosts:
            - *uri
          secretName: *uri
    
    nodeSelector:
      node-role.kubernetes.io/worker: "true"

    probes:
      liveness: 
        enabled: false
        custom: true
        spec:
          httpGet:
            path: /healthz
            port: *port
          initialDelaySeconds: 0
          periodSeconds: 30 
          timeoutSeconds: 1
          failureThreshold: 3
      readiness: 
        enabled: false
        custom: true
        spec:
          httpGet:
            path: /readyz
            port: *port
          initialDelaySeconds: 0
          periodSeconds: 30 
          timeoutSeconds: 1
          failureThreshold: 3
      startup:
        enabled: false

    resources:
      requests:
        cpu: 200m
        memory: 2000Mi
      limits:
        memory: 40000Mi
